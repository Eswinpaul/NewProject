{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtYPBFuVOiDn1PPx/YYjo1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eswinpaul/NewProject/blob/main/NLP_ResearchPaper_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "\n",
        "This notebook demonstrates how to classify research papers based on their abstracts. Natural Language Processing (NLP) and Machine Learning (ML) algorithms are used to achieve this."
      ],
      "metadata": {
        "id": "dr0_oxuX3v0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Required Functions and Modules\n",
        "\n",
        "Before running the code cells, you will need to import the following functions and modules:\n",
        "\n",
        "- `numpy` for numerical calculations\n",
        "- `pandas` for data manipulation\n",
        "- `matplotlib` for data visualization\n",
        "- `sklearn` for machine learning algorithms\n",
        "- `nltk` for NLP tasks\n",
        "- `string` for string operations\n",
        "- `contractions` for expanding contractions in text data\n",
        "\n",
        "You can import these modules by running the following code:\n",
        "\n",
        "'''\n",
        "!pip install -r requirements.txt\n",
        "'''"
      ],
      "metadata": {
        "id": "0MV-8FDY1vWL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II4fBV78GshM",
        "outputId": "9b6033b2-7131-4985-8d30-952920c0c863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 KB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-2.0.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.5/104.5 KB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-multilearn\n",
            "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy\n",
        "!pip install scikit-learn\n",
        "import pandas as pd\n",
        "nltk.download('popular')\n",
        "!pip install contractions\n",
        "import contractions\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "!pip install scikit-multilearn\n",
        "from sklearn.svm import SVC\n",
        "from skmultilearn.problem_transform import LabelPowerset\n",
        "from sklearn.metrics import hamming_loss, accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Loading**\n",
        "Load the data from a CSV file\n",
        "\n",
        "The link to the Data is given below\n",
        "https://www.kaggle.com/datasets/jainpooja/topic-modeling-for-research-articles-20\n",
        "\n",
        "I have trimmed the data into Abstract and the four columns Computer Science, Physics, Mathematis and Statistics"
      ],
      "metadata": {
        "id": "0Qmq4taH40Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data = pd.read_csv(\"Train_data.csv\")"
      ],
      "metadata": {
        "id": "fHC49uR3HIla"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**\n",
        "\n",
        "- Clean the data by removing punctuations, stopwords and numbers.\n",
        "- Expand contractions and tokenize the text.\n",
        "- Performs lemmatization on the tokens and a mapping from part-of-speech tags to WordNet tag names.\n"
      ],
      "metadata": {
        "id": "Ud3Z5LlK8hWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_Lemmatized = WordNetLemmatizer()\n",
        "tag_map = defaultdict(lambda : wn.NOUN)\n",
        "tag_map['J'] = wn.ADJ\n",
        "tag_map['V'] = wn.VERB\n",
        "tag_map['R'] = wn.ADV\n",
        "\n",
        "def clean_text(text):\n",
        "  translation_table = str.maketrans(\"\", \"\", string.punctuation)\n",
        "  text = contractions.fix(str(text))\n",
        "  text = word_tokenize(text)\n",
        "  text = [token.lower() for token in text]\n",
        "  text = [s.translate(translation_table) for s in text]\n",
        "  text = [i for i in text if not i.isnumeric()]\n",
        "  text = [w for w in text if len(w)>1]\n",
        "  text = [word for word in text if word not in stopwords.words(\"english\")]\n",
        "  text =\" \".join([word_Lemmatized.lemmatize(word,tag_map[tag[0]]) for word,tag in pos_tag(text)])\n",
        "  return text\n",
        "\n",
        "Data[\"ABSTRACT\"] = Data[\"ABSTRACT\"].apply(clean_text)"
      ],
      "metadata": {
        "id": "XeqeR9zMUJNm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Split the dataset**\n",
        "- Split the data into training and test sets\n"
      ],
      "metadata": {
        "id": "1bQjUu9s_mIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = Data[\"ABSTRACT\"].values\n",
        "\n",
        "labels = Data.drop([\"ABSTRACT\",\"id\"],axis = 1).values\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,labels, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "NM1qEj03_fPf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Extraction**"
      ],
      "metadata": {
        "id": "dtZtbkzo_1it"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Tfidf_vect = TfidfVectorizer(max_features = 2000, ngram_range=(1,3))\n",
        "\n",
        "\n",
        "X_train_tfidf = Tfidf_vect.fit_transform(X_train).toarray()\n",
        "\n",
        "\n",
        "X_test_tfidf = Tfidf_vect.transform(X_test).toarray()\n",
        "\n",
        "feature_names = Tfidf_vect.get_feature_names()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vjlr-mNPiZjN",
        "outputId": "9a221de1-aae0-4006-86b6-ea0e33721c16"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Building**\n",
        "Build a multilabel classification model using Label Powerset and decision tree classification model using the preprocessed data."
      ],
      "metadata": {
        "id": "vm8_1Y4QC0Vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = LabelPowerset(DecisionTreeClassifier())\n",
        "classifier.fit(X_train_tfidf,y_train)\n",
        "predictions = classifier.predict(X_test_tfidf) \n",
        "predictions.toarray()[1000]"
      ],
      "metadata": {
        "id": "sLrc-a2TlIgv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffe5e16e-3262-41a8-bbac-6dddf38e920e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Evaluation**"
      ],
      "metadata": {
        "id": "6iFI874FDXMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(hamming_loss(y_test,predictions))\n",
        "print(accuracy_score(y_test,predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3zTiV6anVaI",
        "outputId": "f31732fb-719f-4606-cfcd-e97e3f3d5b62"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.18449805279099957\n",
            "0.5616616183470359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xtest = [\"The atmospheric greenhouse effect, an idea that many authors trace back to the traditional works of Fourier (1824), Tyndall (1861), and Arrhenius (1896), and which is still supported in global climatology, essentially describes a fictitious mechanism, in which a planetary atmosphere acts as a heat pump driven by an environment that is radiatively interacting with but radiatively equilibrated to the atmospheric system. According to the second law of thermodynamics, such a planetary machine can never exist. Nevertheless, in almost all texts of global climatology and in a widespread secondary literature, it is taken for granted that such a mechanism is real and stands on a firm scientific foundation. In this paper, the popular conjecture is analyzed and the underlying physical principles are clarified. By showing that (a) there are no common physical laws between the warming phenomenon in glass houses and the fictitious atmospheric greenhouse effects, (b) there are no calculations to determine an average surface temperature of a planet, (c) the frequently mentioned difference of 33° is a meaningless number calculated wrongly, (d) the formulas of cavity radiation are used inappropriately, (e) the assumption of a radiative balance is unphysical, (f) thermal conductivity and friction must not be set to zero, the atmospheric greenhouse conjecture is falsified.\"]\n",
        "xclean = clean_text(xtest)\n",
        "xtest_cleaned = [xclean]\n",
        "xtest_tfidf = Tfidf_vect.transform(xtest_cleaned).toarray()\n",
        "pred = classifier.predict(xtest_tfidf)"
      ],
      "metadata": {
        "id": "SnDlvrRknfkw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred.toarray()"
      ],
      "metadata": {
        "id": "8ZhQ5QqavcU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89333b20-a596-4cd9-fe47-cf27eaa11dc4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 1, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tS0-uSrVJCL",
        "outputId": "75c8b866-dee3-4f65-fc0b-b1e8b3f2c4e2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'ABSTRACT', 'Computer Science', 'Mathematics', 'Physics',\n",
              "       'Statistics'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DkyblPtIVMd_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}